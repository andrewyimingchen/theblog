[
  {
    "path": "posts/welcome/",
    "title": "Biden vs Trump",
    "description": "A text analysis on Biden's and Trump's Twitter account.",
    "author": [
      {
        "name": "Andrew Chen",
        "url": "https://www.linkedin.com/in/andrewyimingchen/"
      }
    ],
    "date": "2020-11-30",
    "categories": [
      "text analysis",
      "twitter analysis",
      "rstats"
    ],
    "contents": "\n\nContents\nGet data\nTweet distribution\nWord frequency\nWordcloud\nOdds ratio\nRetweets and likes\nChanges in word use\nWhole lotta sentiments\nConnecting words\nConclusion\nReference\n\n\n\n\nGet data\nThe datasource for Trump’s Twitter is from this amazing Twitter monitor website built by Brendan Brown, https://www.thetrumparchive.com/, and Biden’s Tiwtter is a Kaggle dataset build by Vopani, https://www.kaggle.com/rohanrao/joe-biden-tweets.\n\n\ntrump_tweets <- read_csv(\"tweets_11-06-2020.csv\")\nbiden_tweets <- read_csv(\"JoeBidenTweets.csv\")\n\n\n\nFirst, we will combine Trump’s and Biden’s Twitter dataset, and we will exclude retweets.\n\n\ntrump_tweets <- trump_tweets %>% \n  mutate(timestamp = date, tweet = text, likes = favorites) %>%\n  filter(!isRetweet) %>%\n  select(id, timestamp, tweet, retweets, likes)\n\nbiden_tweets <- biden_tweets %>% \n  select(-url, -replies, -quotes)\n\nbt_tweets <- bind_rows(trump_tweets %>%\n                         mutate(person = \"Trump\"),\n                       biden_tweets %>%\n                         mutate(person = \"Biden\")) %>%\n  mutate(timestamp = ymd_hms(timestamp))\n\nsummary(bt_tweets)\n\n\n       id              timestamp                      tweet          \n Min.   :3.614e+08   Min.   :2007-10-24 22:45:00   Length:51819      \n 1st Qu.:4.040e+17   1st Qu.:2013-11-22 20:27:13   Class :character  \n Median :6.646e+17   Median :2015-11-11 23:02:31   Mode  :character  \n Mean   :7.425e+17   Mean   :2016-06-13 01:01:13                     \n 3rd Qu.:1.144e+18   3rd Qu.:2019-06-26 18:41:20                     \n Max.   :1.325e+18   Max.   :2020-11-06 17:38:17                     \n    retweets          likes            person         \n Min.   :     0   Min.   :      0   Length:51819      \n 1st Qu.:    41   1st Qu.:     40   Class :character  \n Median :   827   Median :   1798   Mode  :character  \n Mean   :  7128   Mean   :  31276                     \n 3rd Qu.: 11586   3rd Qu.:  50812                     \n Max.   :408866   Max.   :1890946                     \n\nTweet distribution\nFrom the graph below you can see that Biden barely tweets and Trump is a lot more active on twitter. The two verticle line represent the announcement of entering presidential election, the first is Trump’s 2016 campaign, the second is Biden’s 2020 campaign.\n\n\ngroup.colors <- c(Trump = colors()[552], Biden = colors()[26])\n\nbt_tweets %>% ggplot(aes(timestamp, fill = person)) +\n  geom_histogram(position = \"identity\", bins = 30, show.legend = FALSE) +\n  geom_vline(xintercept = c(as.POSIXct(\"2015-06-16\", tz = \"UTC\"), as.POSIXct(\"2019-04-25\", tz = \"UTC\"))) +\n  scale_fill_manual(values = group.colors)+\n  facet_wrap(~person, ncol = 1)\n\n\n\n\nWord frequency\nIn order to make a tidy dataframe ready for text anaylsis in the tweets, which we will remove retweets, the common English stop words, and any http links.\n\n\nremove_reg <- \"&amp;|&lt;|&gt;\" # to remove & < >\nbt_tweets_tidy<- bt_tweets %>%\n  filter(!str_detect(tweet, \"^RT\")) %>% # to remove retweet\n  mutate(text = str_remove_all(tweet, remove_reg),\n         text = str_remove_all(tweet, \"\\\\s?(f|ht)(tp)(s?)(://)([^\\\\.]*)[\\\\.|/](\\\\S*)\")) %>% # to remove http links\n  unnest_tokens(word, text, token = \"tweets\") %>%\n  filter(!word %in% stop_words$word, # to remove stop words \n         !word %in% str_remove_all(stop_words$word, \"'\"), # to remove ' in word\n         !word %in% c(\"amp\", \"lt\", \"gt\"), # remove amp/lt/gt in word\n         str_detect(word, \"[a-z]\")) \n\n\n\nNow we trying to create a dataframe that count each words’ frequency for each person, first, we group by pearson, then count each words used by each person, then left join by a column with total words used by each person, then we can mutate a new column for frequency of each words.\n\n\nfrequ <- bt_tweets_tidy %>%\n  group_by(person) %>%\n  count(word, sort = TRUE) %>%\n  left_join(bt_tweets_tidy %>%\n              group_by(person) %>%\n              summarise(total = n())) %>%\n  mutate(freq = n/total)\nfrequ\n\n\n# A tibble: 53,915 x 5\n# Groups:   person [2]\n   person word                 n  total    freq\n   <chr>  <chr>            <int>  <int>   <dbl>\n 1 Trump  @realdonaldtrump  8121 384723 0.0211 \n 2 Trump  trump             5075 384723 0.0132 \n 3 Trump  president         3036 384723 0.00789\n 4 Trump  people            2941 384723 0.00764\n 5 Trump  country           2054 384723 0.00534\n 6 Trump  america           1850 384723 0.00481\n 7 Trump  donald            1778 384723 0.00462\n 8 Trump  time              1670 384723 0.00434\n 9 Trump  news              1511 384723 0.00393\n10 Trump  obama             1476 384723 0.00384\n# … with 53,905 more rows\n\nfrequ <- frequ %>% select(person, word, freq) %>%\n          spread(person, freq) %>%\n          arrange(Biden, Trump)\n\n\n\nFrom this graph below, the words show up near the read line indicate similar frequency in both Twitter account, the points toward the top mean the words show up more in Trump’s Twitter account, while the points toward the right mean the words show up more in Biden’s Twitter account. From this graph alone, we can already spot some vocabulary differances, such as “fake” & “gender”\n\n\nfrequ %>% ggplot(aes(Biden, Trump)) + \n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +\n  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  geom_abline(color = \"red\")\n\n\n\n\nHowever, Biden’s Twitter account is barely active prior to his announcement of 2020 election, so let’s see the frequency plot after 2019/04/25. Again, you see “fake” still falls above the redline and “climate” at the bottem right.\n\n\nbt_tweets_tidy_campaign <- bt_tweets_tidy %>% filter(timestamp >= as.Date(\"2019-04-25\"))\nfrequ_2019 <- bt_tweets_tidy_campaign %>%\n  group_by(person) %>%\n  count(word, sort = TRUE) %>%\n  left_join(bt_tweets_tidy %>%\n              group_by(person) %>%\n              summarise(total = n())) %>%\n  mutate(freq = n/total)\n\nfrequ_2019 <- frequ_2019 %>% select(person, word, freq) %>%\n  spread(person, freq) %>%\n  arrange(Biden, Trump)\n\n\n\n\n\nfrequ_2019 %>% ggplot(aes(Biden, Trump)) + \n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +\n  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  geom_abline(color = \"red\")\n\n\n\n\nWordcloud\n1. Total word counts\nThis graph represents the overall word counts in the dataset, closer to the center means that particular word appear the more than the other and the size of the word shows the magnitude of the word appearance, in this case “trump” and “president” are the most used word.\n\n\nbt_tweets_tidy %>%\n  group_by(word) %>%\n  filter(!str_detect(word, \"^@\")) %>%\n  count() %>%\nwith(wordcloud(word, n, min.freq = 200, max.word = 400, rot.per =0.35, random.order = FALSE,\n          colors = brewer.pal(12, \"Paired\"), scale = c(3,1)))\n\n\n\n\n2. Hashtags\nThese two graphs represent the hastag counts in Trump’s and Biden’s Twitter accounts, not surprisingly Trump’s most used hashtag is his 2016 presidential campaign “#trump2016”, whereas Biden is Democrat’s debate “#demdebate”, I would expect “teamjoe” be the most used hashtag.\n\n\nbt_tweets_tidy %>%\n  filter(person == \"Trump\") %>%\n  group_by(word) %>%\n  filter(str_detect(word, \"#\")) %>%\n  count() %>%\n  with(wordcloud(word,n, min.freq = 1, max.word = 300, rot.per =0.35, random.order = FALSE,\n                 colors = brewer.pal(12, \"Paired\"), scale = c(3,0.8)))\n\n\n\n\n\n\nbt_tweets_tidy %>%\n  filter(person == \"Biden\") %>%\n  group_by(word) %>%\n  filter(str_detect(word, \"#\")) %>%\n  count() %>%\n  with(wordcloud(word,n, min.freq = 1, max.word = 300, rot.per =0.35, random.order = FALSE,\n                 colors = brewer.pal(12, \"Paired\"), scale = c(3,0.8)))\n\n\n\n\n3. Separation in sentiments\nHere I tried a slightly fancier word cloud that separate the words base on the sentiments in NRC, https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html, graph 1 represents the sentiment word counts in Trump’s account, and graph 2 represents Biden’s. “vote”, “president” and “time” are all quite relevant in both account, but why “vote” is assigned to surprise is beyond my understanding.\n\n\n# trump words\nbt_tweets_tidy %>%\n  filter(person == \"Trump\" & !word == \"trump\") %>%\n  inner_join(get_sentiments(\"nrc\")) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  acast(word ~ sentiment, value.var = \"n\", fill =0) %>%\n  comparison.cloud(colors = brewer.pal(10, \"Paired\"), max.words = 400, \n                   match.colors = TRUE, title.size = 1.5, random.order = FALSE, scale = c(3,0.5))\n\n\n\n\n\n\n# biden words \nbt_tweets_tidy %>%\n  filter(person == \"Biden\" & !word == \"trump\") %>%\n  inner_join(get_sentiments(\"nrc\")) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  acast(word ~ sentiment, value.var = \"n\", fill =0) %>%\n  comparison.cloud(colors = brewer.pal(10, \"Paired\"), max.words = 400, \n                   match.colors = TRUE, title.size = 1.5, random.order = FALSE, scale = c(3,0.5))\n\n\n\n\nOdds ratio\nIn order to know which word is more likely to be used in either Twitter account, we will find the odds ratio for each words and here we use the timeframe after Biden announced his presidential campaign. After ungroup, we spread the data by person.\n\n\nbt_words_ratio <- bt_tweets_tidy_campaign %>%\n  filter(!str_detect(word, \"^@\")) %>% # remove tags on individual accounts \n  count(word, person) %>%\n  group_by(word) %>%\n  filter(sum(n) >= 10) %>% # only keep the words that are used more than 10 times \n  ungroup() %>%\n  spread(person, n, fill = 0) %>%\n  mutate_if(is.numeric, list(~(. + 1) / (sum(.) + 1))) %>%\n  mutate(logratio = log(Biden / Trump)) %>%\n  arrange(desc(logratio))\n\n\n\nHere it’s just a demonstration of how to calculate the odds ratio manunally.\n\n\nbt_tweets_tidy_campaign %>%\n  filter(!str_detect(word, \"^@\")) %>%\n  count(word, person) %>%\n  group_by(word) %>%\n  filter(sum(n) >= 10) %>%\n  ungroup() %>% # use ungroup here is because if use summarise the data would automatically reduce the dimension\n  spread(person, n, fill = 0) %>%\n  mutate(biden_t = sum(Biden),\n         trump_t = sum(Trump),\n         br = (Biden+1)/(biden_t+1), \n         tr = (Trump+1)/(trump_t+1), \n         logr = log(br/tr)) %>%\n  arrange(abs(logr))\n\n\n# A tibble: 2,488 x 8\n   word      Biden Trump biden_t trump_t       br       tr     logr\n   <chr>     <dbl> <dbl>   <dbl>   <dbl>    <dbl>    <dbl>    <dbl>\n 1 recovery     20    30   49467   72935 0.000425 0.000425 -0.00121\n 2 lot          43    64   49467   72935 0.000889 0.000891 -0.00194\n 3 words        44    66   49467   72935 0.000910 0.000919 -0.00977\n 4 happening    34    50   49467   72935 0.000708 0.000699  0.0118 \n 5 forgotten    10    15   49467   72935 0.000222 0.000219  0.0136 \n 6 july         10    15   49467   72935 0.000222 0.000219  0.0136 \n 7 pushing      10    15   49467   72935 0.000222 0.000219  0.0136 \n 8 refuse       10    15   49467   72935 0.000222 0.000219  0.0136 \n 9 task         10    15   49467   72935 0.000222 0.000219  0.0136 \n10 fighting     50    73   49467   72935 0.00103  0.00101   0.0160 \n# … with 2,478 more rows\n\nFrom the log odds ratio graph, I pick the top 15 words in both Twitter accounts, not surprisingly Biden’s account is more likely to use vocabulary such as “climate”, “gender”, “lgbtq”, “inclusive”, where as Trump’s account used a vocabulary like “wow”, “lamestream”, “sleepy”, “fake”, and most obviously “#maga”.\n\n\n## the words more likely from the otherside \ntratio <- bt_words_ratio %>%\n  group_by(logratio < 0) %>%\n  top_n(15, abs(logratio)) %>%\n  ungroup() %>%\n  mutate(word = reorder(word, logratio)) %>%\n  filter(logratio<0)\n\nbratio <- bt_words_ratio %>%\n  group_by(logratio < 0) %>%\n  top_n(15, abs(logratio)) %>%\n  ungroup() %>%\n  mutate(word = reorder(word, logratio)) %>%\n  filter(logratio>0)  \n\nsubplot(plot_ly(bratio, x=~logratio, y=~word, type = \"bar\", name = \"Biden\", color = I(\"blue\")),\n        plot_ly(tratio, x=~logratio, y=~word, type = \"bar\", name = \"Trump\", color = I(\"red\"), yaxis=\"y2\"),\n        shareX = TRUE) %>%\n  layout(legend = list(x = 100, y = 0.5),\n         title = \"log odds ratio (Biden/Trump)\",\n         width = 800) %>%\n  theme_plotly()\n\n\npreserve31deab9c9d99b807\n\nRetweets and likes\n1. Retweets\nLet’s see what sort of words that give the highest retweets for Trump and Biden. First, check the total rtweets from both accounts. The first group_by() and summarise() count how many retweets of each tweets id, the second group_by() and sumarise() sum up the whole retweets for each account.\n\n\nbt_totals <- bt_tweets_tidy %>%\n  group_by(person, id) %>%\n  summarise(rts = first(retweets)) %>%\n  group_by(person) %>%\n  summarise(total_rts = sum(rts))\n\nbt_totals\n\n\n# A tibble: 2 x 2\n  person total_rts\n  <chr>      <dbl>\n1 Biden   37778753\n2 Trump  306628537\n\nNow we have the total retweets ready, we want to find the median of retweets of each word. The frist group_by() and summarise() show how many retweets does each word has for each tweet id and person. The second group_by() and summarise() show the median or retweets for each word and the total usage of each word. (so if the word is used only once, the median retweets is the same as the maximum and minum retweets.)\n\n\nbt_word_by_rts <- bt_tweets_tidy %>%\n  group_by(id, person, word) %>%\n  summarise(rts = first(retweets)) %>%\n  group_by(person, word) %>%\n  summarise(retweets = median(rts), uses =n()) %>%\n  left_join(bt_totals) %>%\n  filter(retweets != 0) %>%\n  ungroup()\n\nbt_word_by_rts\n\n\n# A tibble: 53,589 x 5\n   person word            retweets  uses total_rts\n   <chr>  <chr>              <dbl> <int>     <dbl>\n 1 Biden  @32bjseiu            263     1  37778753\n 2 Biden  @60minutes           652     1  37778753\n 3 Biden  @abby4iowa          1000     1  37778753\n 4 Biden  @abc                9296     1  37778753\n 5 Biden  @abcnetwork          751     1  37778753\n 6 Biden  @abeshinzo          7127     1  37778753\n 7 Biden  @adamsmithtimes       19     1  37778753\n 8 Biden  @adybarkan          6589     1  37778753\n 9 Biden  @aflcio              423     1  37778753\n10 Biden  @afscme               46     1  37778753\n# … with 53,579 more rows\n\nThe following graph shows the median number of retweets for each word after filter the use for at least 5 times. It’s funny that words related to A$AP Rocky incident in Sweden are the most retweets for Trump, in the other hand Biden’s words are link with COVID19.\n\n\nsub_rtt <- bt_word_by_rts %>%\n  filter(uses >= 5 &\n           person == \"Trump\") %>%\n  group_by(person) %>%\n  top_n(10, retweets) %>%\n  arrange(retweets) %>%\n  ungroup() %>%\n  mutate(word = factor(word, unique(word))) %>%\n  ungroup()\n\nsub_rtb <- bt_word_by_rts %>%\n  filter(uses >= 5 &\n           person == \"Biden\") %>%\n  group_by(person) %>%\n  top_n(10, retweets) %>%\n  arrange(retweets) %>%\n  ungroup() %>%\n  mutate(word = factor(word, unique(word))) %>%\n  ungroup()\n\nsubplot(sub_rtt %>%\n          plot_ly(x=~retweets, y=~word, type = \"bar\", color = I(\"red\"), name = \"Trump\"),\n        sub_rtb %>%\n          plot_ly(x=~retweets, y=~word, type = \"bar\", color =I(\"blue\"), name = \"Biden\")) %>%\n  layout(title = \"Median number of retweets\",\n         legend = list(x = 100, y = 0.5),\n         width = 850) %>%\n         theme_plotly()\n\n\npreserve7b8d5dc70d818a0a\n\n2. Likes\nThe strategy to get check the what word gives the highest median of retweets is the same as the one we used above.\n\n\nbt_totals_fav <- bt_tweets_tidy %>%\n  group_by(person, id) %>%\n  summarise(fav = first(likes)) %>%\n  group_by(person) %>%\n  summarise(total_favs = sum(fav))\n\nbt_totals_fav\n\n\n# A tibble: 2 x 2\n  person total_favs\n  <chr>       <dbl>\n1 Biden   206517140\n2 Trump  1320390366\n\nThe results are more or less the same as retweets in both accounts, where words relate to A$ap Rocky earned the most likes for Trump, and COVID19 related words got the most likes for Biden.\n\n\nbt_word_by_fav <- bt_tweets_tidy %>%\n  group_by(id, person, word) %>%\n  summarise(fav = first(likes)) %>%\n  group_by(person, word) %>%\n  summarise(favs = median(fav), uses =n()) %>%\n  left_join(bt_totals_fav) %>%\n  filter(favs != 0) %>%\n  ungroup()\n\nbt_word_by_fav\n\n\n# A tibble: 53,149 x 5\n   person word             favs  uses total_favs\n   <chr>  <chr>           <dbl> <int>      <dbl>\n 1 Biden  @32bjseiu        1314     1  206517140\n 2 Biden  @60minutes       3705     1  206517140\n 3 Biden  @abby4iowa       6152     1  206517140\n 4 Biden  @abc            89310     1  206517140\n 5 Biden  @abcnetwork      7303     1  206517140\n 6 Biden  @abeshinzo      39912     1  206517140\n 7 Biden  @adamsmithtimes     7     1  206517140\n 8 Biden  @adybarkan      33607     1  206517140\n 9 Biden  @aflcio          1986     1  206517140\n10 Biden  @afscme             7     1  206517140\n# … with 53,139 more rows\n\n\n\nsub_trump <- bt_word_by_fav %>%\n  filter(uses >= 5 &\n         person == \"Trump\") %>%\n  group_by(person) %>%\n  top_n(10, favs) %>%\n  arrange(favs) %>%\n  ungroup() %>%\n  mutate(word = factor(word, unique(word))) %>%\n  ungroup()\n  \nsub_biden <- bt_word_by_fav %>%\n  filter(uses >= 5 &\n           person == \"Biden\") %>%\n  group_by(person) %>%\n  top_n(10, favs) %>%\n  arrange(favs) %>%\n  ungroup() %>%\n  mutate(word = factor(word, unique(word))) %>%\n  ungroup()\n\nsubplot(sub_trump %>%\n          plot_ly(x=~favs, y=~word, type = \"bar\", color = I(\"red\"), name = \"Trump\"),# %>%\n          #layout(legend = list(title = \"Trump's \")),\n        sub_biden %>%\n          plot_ly(x=~favs, y=~word, type = \"bar\", color =I(\"blue\"), name = \"Biden\")) %>%\n        layout(title = \"Median number of likes\",\n               legend = list(x = 100, y = 0.5),\n               width = 850) %>%\n        theme_plotly()\n\n\npreserve121454c2ffc7cc8b\n\nChanges in word use\n1. Data prepping\nSo now we would look at how words’ frequencies change overtime. To do that, we first create a new time variable to get the unit of time to calculate the word in that time span. Here, I choose 1 month as the base, in floor_date() you can set your unit of time in different measures. The count() counts how many times the word is use in 1 month then follow by a group_by() to account unit of time in order to use a mutate() to attach the total amount of word used in 1 month, the second group_by() accounts word so that mutate() can attach a total count of word use by that person.\n\n\nbt_words_by_time <- bt_tweets_tidy_campaign %>%\n  filter(!str_detect(word, \"^@\")) %>% # to filter out account taging in Twitter\n  mutate(time_floor = floor_date(timestamp, unit = \"1 month\")) %>%\n  count(time_floor, person, word) %>%\n  group_by(person, time_floor) %>%\n  mutate(time_total = sum(n)) %>%\n  group_by(person, word) %>%\n  mutate(word_total = sum(n)) %>%\n  ungroup() %>%\n  rename(count = n) %>%\n  filter(word_total > 50)\n\n\n\n2. Nested Model\nNow the dataset is ready to be nested to run our model. Since we trying to find if the word useage change over time, it’s ideal to use a nested dataset, so in each word it nested a samller dataset that contains the “time_floor”, “count”, “time_total”, and “word_total”. Here we use glm and binomial for our model.\n\n\nbt_nested_data <- bt_words_by_time %>%\n  group_by(person, word) %>%\n  nest()\n\nbt_nested_data\n\n\n# A tibble: 507 x 3\n# Groups:   person, word [507]\n   person word       data             \n   <chr>  <chr>      <list>           \n 1 Biden  act        <tibble [19 × 4]>\n 2 Biden  address    <tibble [16 × 4]>\n 3 Biden  affordable <tibble [19 × 4]>\n 4 Biden  america    <tibble [20 × 4]>\n 5 Biden  american   <tibble [20 × 4]>\n 6 Biden  americans  <tibble [20 × 4]>\n 7 Biden  battle     <tibble [18 × 4]>\n 8 Biden  bring      <tibble [16 × 4]>\n 9 Biden  build      <tibble [20 × 4]>\n10 Biden  campaign   <tibble [19 × 4]>\n# … with 497 more rows\n\nmodel <- function(x){\n  glm(cbind(count, time_total) ~ time_floor, data = x, family = \"binomial\")\n}\n\n# map(bt_nested_data$data, model), I forgot how to use map() so put it here for reference \n\nbt_nested_models <- bt_nested_data %>%\n  mutate(models = map(data, model))\n\nbt_nested_models\n\n\n# A tibble: 507 x 4\n# Groups:   person, word [507]\n   person word       data              models\n   <chr>  <chr>      <list>            <list>\n 1 Biden  act        <tibble [19 × 4]> <glm> \n 2 Biden  address    <tibble [16 × 4]> <glm> \n 3 Biden  affordable <tibble [19 × 4]> <glm> \n 4 Biden  america    <tibble [20 × 4]> <glm> \n 5 Biden  american   <tibble [20 × 4]> <glm> \n 6 Biden  americans  <tibble [20 × 4]> <glm> \n 7 Biden  battle     <tibble [18 × 4]> <glm> \n 8 Biden  bring      <tibble [16 × 4]> <glm> \n 9 Biden  build      <tibble [20 × 4]> <glm> \n10 Biden  campaign   <tibble [19 × 4]> <glm> \n# … with 497 more rows\n\n### different way to run glm over nested data\n# bt_nested_models <- bt_nested_data %>%\n#  mutate(models = map(data, ~glm(cbind(count, time_total) ~ time_floor, ., family = \"binomial\")))\n###\n\n\n\nFrom above the bt_nested_models is has another list attach in the dataframe label as models, now we want to extract the componets in the glm model, here we can use a powerful function tidy() to summarize information in the model then unnest that models column. Since we are comparing multiple p values best to adjust them!\n\n\nslopes <- bt_nested_models %>%\n  mutate(models = map(models, tidy)) %>%\n  unnest(cols = c(models)) %>%\n  filter(term == \"time_floor\") %>%\n  mutate(adjusted.p.value = p.adjust(p.value))\n\ntop_slopes <- slopes %>% filter(adjusted.p.value <0.05)\n\n\n\n3. Graphing\nFrom the graphs below, both illustrate the trendy words that appeared more than 350 times in the period of 2019-04-25 to now. In the trump graph, it is obvious that closer to the election date the mentioning of biden and vote skyrocket compare to other words. In the other hand, biden graph shows that “trump” and “president” maintian rather stable high freqencies.\n\n\nbt_words_by_time %>%\n  inner_join(top_slopes, by = c(\"word\", \"person\")) %>%\n  filter(person == \"Trump\" &\n           word_total > 350) %>%\n  plot_ly() %>%\n  add_lines(x=~time_floor, y=~count/time_total, color = ~word, mode = \"line\", type = \"scatter\") %>%\n  layout(\n    title = \"Trump's word trends\",\n    xaxis = list(title=\"Time\"),\n    yaxis = list(title=\"Frequency\"),\n    legend= list(title=list(text='<b> Word <\/b>'),\n                 x = 100, y = 0.5),\n    width = 800\n  ) %>%\n  theme_plotly()\n\n\npreserveac14c57ae8042f0d\n\nbt_words_by_time %>%\n  inner_join(top_slopes, by = c(\"word\", \"person\")) %>%\n  filter(person == \"Biden\" &\n           word_total > 350) %>%\n  plot_ly() %>%\n  add_lines(x=~time_floor, y=~count/time_total, color = ~word, mode = \"line\", type = \"scatter\") %>%\n  layout(\n    title = \"Biden's word trends\",\n    xaxis = list(title=\"Time\"),\n    yaxis = list(title=\"Frequency\"),\n    legend= list(title=list(text='<b> Word <\/b>'),\n                 x = 100, y = 0.5),\n    width = 800\n  ) %>%\n  theme_plotly()\n\n\npreserve76c16ca0b0d968d1\n\nWhole lotta sentiments\nWe briefly touch on sentiments in the wordcloud section, here we going to add in more sentiment analysis. First, I want to check what is the overall change of sentiment through time in both Twitter accounts. Second, I will show what are the vocabularies they use corrorspond to the positive and negative sentiments. Last, I will inspect the aggregate positive and negative sentiment over time.\n1.Data prepping\nSince we want to see the how sentiment shifts since the beginning of the first tweet, we need to make some changes to the original bt_tweets_tidy dataset above. I arrange the dataset according to timestamp then create an index for each row, and here we used a custom stopword “trump” to filter out trump, because Bing sentiment take “trump” as an positive vocabulary, which would show overly positive bias in both account, becuase they both mentioned “Trump” a lot of times.\n\n\ncustom_stop_words <- bind_rows(tibble(word = c(\"trump\"), lexicon = c(\"custom\")), stop_words)\n\nbt_tweets_tidy<- bt_tweets %>%\n  arrange(timestamp) %>%\n  mutate(tweetnumber = row_number()) %>%\n  filter(!str_detect(tweet, \"^RT\")) %>%\n  mutate(text = str_remove_all(tweet, remove_reg),\n         text = str_remove_all(tweet, \"\\\\s?(f|ht)(tp)(s?)(://)([^\\\\.]*)[\\\\.|/](\\\\S*)\")) %>%\n  unnest_tokens(word, text, token = \"tweets\") %>%\n  filter(!word %in% custom_stop_words$word,\n         !word %in% str_remove_all(custom_stop_words$word, \"'\"),\n         !word %in% c(\"amp\", \"lt\", \"gt\"),\n         str_detect(word, \"[a-z]\"))\n\n\n\nBefore we get into graphing, we will need to prepare the dataset into a format we wanted. The procedure is fairly simply here, we left_join() each sentiment separately then we combine them together by rows using bind_rows(), here the index is create by uing %/%, which gaves you only the quotient, so the index take into account of 150 tweets as a bin to count the sentiment. One thing to keep in mind is that Afinn sentiment assigned words with a range of value from -5 to 5 (which is called value insted of sentiment, hence I renamed it to sentiment).\nBing sentiment is categorize in binary section of positive and negative, but Nrc has 10 different sentiments, therefore I filter for the wanted sentiments. After combining Bing and Nrc, there is an extra procedure to account for the overall sentiments of each index, which I achieved this by subtracting negative from positive.\n\n\nbt_afinn <- bt_tweets_tidy %>% \n  inner_join(get_sentiments(\"afinn\")) %>% \n  group_by(person, index = tweetnumber %/% 150) %>% \n  summarise(sentiment = sum(value)) %>% \n  mutate(method = \"AFINN\")\n\nbt_bing_and_nrc <- bind_rows(\n  bt_tweets_tidy %>% \n    inner_join(get_sentiments(\"bing\")) %>%\n    mutate(method = \"Bing\"),\n  bt_tweets_tidy %>% \n    inner_join(get_sentiments(\"nrc\") %>% \n                 filter(sentiment %in% c(\"positive\", \n                                         \"negative\"))\n    ) %>%\n    mutate(method = \"NRC\")) %>%\n  count(person, method, index = tweetnumber %/% 150, sentiment) %>%\n  spread(sentiment, n, fill = 0) %>%\n  mutate(sentiment = positive - negative)\n\nbt_all_sent <- bind_rows(bt_afinn, bt_bing_and_nrc)\n\n\n\n2. Change of sentiment over time\nNow the dataset is ready to do some graph! From the graphs, Afinn and Bing both show simialr trends, however Nrc sentiment is over whelminingly positive, We can do a little check of that account. As you can see from the Nrc and Bing tibbles, you can tell NRC has less negative words and more positive words compare to Bing, which is probably the major reason why both Trump and Biden have a lot more positives sentiment when using Nrc measurement.\n\n\nbt_all_sent %>% filter(person == \"Biden\") %>% \n  ggplot(aes(index, sentiment, fill = method)) + \n  geom_col(show.legend = FALSE) +\n  facet_wrap(~method, ncol = 1, scales = \"free_y\")\n\n\n\nbt_all_sent %>% filter(person == \"Trump\") %>% \n  ggplot(aes(index, sentiment, fill = method)) + \n  geom_col(show.legend = FALSE) +\n  facet_wrap(~method, ncol = 1, scales = \"free_y\")\n\n\n\n# checking the difference of NRC and Bing \nget_sentiments(\"nrc\") %>% \n  filter(sentiment %in% c(\"positive\", \"negative\")) %>% \n  count(sentiment)\n\n\n# A tibble: 2 x 2\n  sentiment     n\n  <chr>     <int>\n1 negative   3324\n2 positive   2312\n\nget_sentiments(\"bing\") %>% \n  count(sentiment)\n\n\n# A tibble: 2 x 2\n  sentiment     n\n  <chr>     <int>\n1 negative   4781\n2 positive   2005\n\n3. Overall cumulative sentiments\nThis is a rather simple method, we just need to take the bt_tweets_tidy which is already prepared for graphing and inner_join() with sentiment. So how is Trump sentiments perform since his announcement of presidential campaign in 2015/06/16? It is quite obvious that over time his tweets are becoming more negative than positive where the difference between the two become negative as shown in the difference line. But what about Biden’s sentiment after his announcement of his 2020 presidential campaign? The graph showed an opposite story, which the positive line diverge from the negative line early on and the positive difference is even larger in later tweets.\n\n\n\n\n\n\n\n\nbt_tweets_sent %>% \n  filter(person == \"Trump\" &\n           !word %in% \"trump\" &\n           timestamp >= \"2015-06-16\") %>%\n  mutate(positivity = cumsum(if_else(value>0, value, 0)),\n         negativity = cumsum(abs(if_else(value<0, value, 0)))) %>% \n  plot_ly() %>% \n  add_lines(x=~tweetnumber, y=~positivity, name='positive') %>% \n  add_lines(x=~tweetnumber, y=~negativity, name='negative', color = I(\"red\")) %>%\n  add_lines(x=~tweetnumber, y=~positivity - negativity, name=\"difference\", yaxis=\"y2\", color = I(\"#ff8400\")) %>% \n  layout(\n    title = \"Trump's overall cumulative sentiment\",\n    yaxis = list(title='absolute cumulative sentiment'),\n    yaxis2 = tb,\n    width = 800\n  ) %>% \n  theme_plotly()\n\n\npreserve7fa56a30454548e9\n\nbt_tweets_sent %>% \n  filter(person == \"Biden\" &\n           !word %in% \"trump\" &\n           timestamp >= \"2019-04-25\") %>%\n  mutate(positivity = cumsum(if_else(value>0, value, 0)),\n         negativity = cumsum(abs(if_else(value<0, value, 0)))) %>% \n  plot_ly() %>% \n  add_lines(x=~tweetnumber, y=~positivity, name='positive') %>% \n  add_lines(x=~tweetnumber, y=~negativity, name='negative', color = I(\"red\")) %>%\n  add_lines(x=~tweetnumber, y=~positivity - negativity, name=\"difference\", yaxis=\"y2\", color = I(\"#ff8400\")) %>% \n  layout(\n    title = \"Biden's overall cumulative sentiment\",\n    yaxis = list(title='absolute cumulative sentiment'),\n    yaxis2 = tb,\n    width = 800\n  ) %>% \n  theme_plotly()\n\n\npreserve21e4cd4dad634f1f\n\n4. What words contribute to sentiments?\nHere, we going to check individual word contribution to the overall sentiment for both Twittier accounts, after the announcement of presidential camgaign. It is as expected that the majority of the negative sentiment in Trump’s account is contributed by “fake”, which is quite obvious by the constant “fake news” tweets, where as the positive sentiment is filled by “win”. As for Biden, “crisis” contributed the most to the negative sentiment, which is the results of COVID19 tweets. “Support” and “protect” contributed to the positive sentiment relatively the same. Later, we will look on detail later on how words associate with eatch other.\n\n\nwordcount_t <- bt_tweets_tidy %>%\n  filter(timestamp >= \"2015-06-16\") %>%\n  inner_join(get_sentiments(\"bing\")) %>%\n  count(person, word, sentiment, sort = TRUE) %>%\n  ungroup()\n\nwordcount_b <- bt_tweets_tidy %>%\n  filter(timestamp >= \"2019-04-25\") %>%\n  inner_join(get_sentiments(\"bing\")) %>%\n  count(person, word, sentiment, sort = TRUE) %>%\n  ungroup()\n\nggplotly(wordcount_t %>%\n           filter(person== \"Trump\" &\n                    !word %in%custom_stop_words$word) %>%\n           group_by(sentiment) %>%\n           top_n(10) %>%\n           ungroup() %>%\n           mutate(word = reorder(word, n)) %>%\n           ggplot(aes(n, word, fill = sentiment)) +\n           geom_col(show.legend = FALSE) +\n           facet_wrap(~sentiment, scales = \"free_y\") +\n           labs(title = \"Trump\",\n                x = \"Contribution to sentiment\",\n                y = NULL)) %>%\n  theme_plotly()\n\n\npreserve6aea6bc6716afbf2\n\nggplotly(wordcount_b %>%\n           filter(person== \"Biden\" &\n                    !word %in%custom_stop_words$word) %>%\n           group_by(sentiment) %>%\n           top_n(10) %>%\n           ungroup() %>%\n           mutate(word = reorder(word, n)) %>%\n           ggplot(aes(n, word, fill = sentiment)) +\n           geom_col(show.legend = FALSE) +\n           facet_wrap(~sentiment, scales = \"free_y\") +\n           labs(title = \"Biden\",\n                x = \"Contribution to sentiment\",\n                y = NULL)) %>%\n  theme_plotly()\n\n\npreservec490e7de9d5c8f99\n\nConnecting words\n1. Ngrams\nWhat if you want to check the what come after the word “ABC”, what should you do? Ngrams would be a good tool in this sort of situation. The data wrangling process is relative the same as the very first section of Get data, unnest_tokens(), but here the token is “ngrams” and we will have to set “n” in order to tell how many consecutive words we want from the text. After unnesting, we will get a dataframe with every two consecutive words combinations, then separate(), filter(), and count() the number of “word1” “word2” combination, last we use graph_from_data_frame() so we could create an igraph (network).\n\n\nbt_bigrams <- bt_tweets %>%\n  unnest_tokens(bigram, tweet, token = \"ngrams\", n = 2) %>%\n  filter(!str_detect(bigram, \"http\") &\n         !str_detect(bigram, \"www.\") &\n         !str_detect(bigram, \".com\") &\n         !str_detect(bigram, \".org$\") &\n         !str_detect(bigram, \"^t.co\") &\n         !str_detect(bigram, \"\\\\d\") &\n         !str_detect(bigram, \"^amp\"))\n\nbigrams_sep <- bt_bigrams %>%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \")\n\nbigrams_filt <- bigrams_sep %>%\n  filter(!word1 %in% stop_words$word) %>%\n  filter(!word2 %in% stop_words$word)\n\nbigrams_graph_t <- bigrams_filt %>%\n  filter(!word2 == \"amp\" &\n         timestamp >= \"2015-06-16\" &\n         person == \"Trump\") %>%\n  count(word1, word2, sort = TRUE) %>%\n  filter(n > 20) %>%\n  graph_from_data_frame()\n\nbigrams_graph_b <- bigrams_filt %>%\n  filter(!word2 == \"amp\" &\n           timestamp >= \"2019-04-25\" &\n           person == \"Biden\") %>%\n  count(word1, word2, sort = TRUE) %>%\n  filter(n > 20) %>%\n  graph_from_data_frame()\n\n\n\nNow we are ready for graphing. In the Markov chain below, the individual node represents each word and the arrow represents the direction, the darker the color of the arrow means that the number of time of the bigrams combination appears in tweets is higher.\n\n\na <- grid::arrow(type = \"closed\", length = unit(.15, \"inches\"))\n\nggraph(bigrams_graph_t, layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,\n                 arrow = a, end_cap = circle(.07, 'inches')) +\n  geom_node_point(color = \"red\", size = 3) +\n  geom_node_text(aes(label = name), vjust = 1, hjust = 1, size = 3) +\n  theme_void() +\n  ggtitle(\"Trump\")\n\n\n\nggraph(bigrams_graph_b, layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,\n                 arrow = a, end_cap = circle(.07, 'inches')) +\n  geom_node_point(color = \"blue\", size = 3) +\n  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +\n  theme_void() +\n  ggtitle(\"Biden\")\n\n\n\n\n2. Correlation between words\nWe touched on the ngrams method to analze consecutive words in a text, now we going to look at the correlation of paired words. We gonna take the bt_tweets_tidy and create a new variable sec_by100 which we bind every words in each 100 tweets of each person. Once the dataset is tidy, we can filter() for timestamp and person. Since we want to assess how likely the paired words appear together, separately, or no appearance, which we can use pairwise_cor() from the widyr package.\n\n\nbt_tweets_tidy<- bt_tweets %>%\n  arrange(timestamp) %>%\n  mutate(sec_by100 = row_number()%/%100) %>%\n  filter(!str_detect(tweet, \"^RT\") &\n         sec_by100 > 0) %>%\n  mutate(text = str_remove_all(tweet, remove_reg),\n         text = str_remove_all(tweet, \"\\\\s?(f|ht)(tp)(s?)(://)([^\\\\.]*)[\\\\.|/](\\\\S*)\")) %>%\n  unnest_tokens(word, text, token = \"tweets\") %>%\n  filter(!word %in% custom_stop_words$word,\n         !word %in% str_remove_all(custom_stop_words$word, \"'\"),\n         !word %in% c(\"amp\", \"lt\", \"gt\"),\n         str_detect(word, \"[a-z]\") &\n         !str_detect(word, \"http\") &\n         !str_detect(word, \"@\") &\n         !str_detect(word, \"cont\") &\n         !str_detect(word, \"www.\") &\n         !str_detect(word, \".com$\") &\n         !str_detect(word, \".org$\") &\n         !str_detect(word, \"^tco\") &\n         !str_detect(word, \"\\\\d\") & \n         !str_detect(word, \"^amp\") &\n         !str_detect(word, \"realdonald\"))\n\nword_cors_t <- bt_tweets_tidy %>%\n  filter(timestamp >= \"2015-06-16\" &\n         person == \"Trump\") %>%\n  group_by(word) %>%\n  filter(n() >= 20) %>%\n  pairwise_cor(word, sec_by100, sort = TRUE)\n\nword_cors_b <- bt_tweets_tidy %>%\n  filter(timestamp >= \"2019-04-25\" &\n           person == \"Biden\") %>%\n  group_by(word) %>%\n  filter(n() >= 20) %>%\n  pairwise_cor(word, sec_by100, sort = TRUE)\n\n\n\nWe are all set for the correlation graph. The method here is the same as the Markov chain graph in the example above, but the major difference is that there is no direction in the paired words, because we are showing how likely the pairs show up in the every 100 tweets. We filtered the correlation to only showing the pairs that have a correlation above 0.5, and the darker the line means the higher the correlation. It is quite interesting that in Trump’s tweets he mentioned a lot of names where as Biden’s tweets was more about policy. I guess from the correlation graph is much more clear how different the two accounts are tweeting about.\n\n\nword_cors_t %>%\n  filter(correlation > .50) %>%\n  graph_from_data_frame() %>%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +\n  geom_node_point(color = \"red\", size = 3) +\n  geom_node_text(aes(label = name), repel = TRUE) +\n  theme_void() +\n  ggtitle(\"Trump\")\n\n\n\nword_cors_b %>%\n  filter(correlation > .50) %>%\n  graph_from_data_frame() %>%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +\n  geom_node_point(color = \"blue\", size = 3) +\n  geom_node_text(aes(label = name), repel = TRUE) +\n  theme_void() +\n  ggtitle(\"Biden\")\n\n\n\n\nConclusion\nThis is my first attemp to do a text analysis and there is a lot more to explore in this section, such as unsupervise learning with LDA. This post is just a show case of what R can achieve in text mining on Twitter to examine more detail on what these politicians are actually saying, obviously text analysis can also be down on books, document or songs. However, I am still a bit skeptical about sentiment analysis because the way these methods could assigned words into odd categories. I might revisit this topic later and make an update in the future when I got more time.\nReference\nText Mining with R [ Julia Silge; David Robinson].\nAn Introduction to Text Processing and Analysis with R [Michael Clark].\n\n\n\n",
    "preview": "posts/welcome/welcome_files/figure-html5/tweet distribution-1.png",
    "last_modified": "2020-12-19T22:05:52+08:00",
    "input_file": "welcome.utf8.md",
    "preview_width": 1248,
    "preview_height": 768
  }
]
