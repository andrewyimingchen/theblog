[
  {
    "path": "posts/welcome/",
    "title": "Twitter Analysis",
    "description": "A text analysis on Biden's and Trump's Twitter account.",
    "author": [
      {
        "name": "Andrew Chen",
        "url": "https://www.linkedin.com/in/andrewyimingchen/"
      }
    ],
    "date": "2020-11-30",
    "categories": [
      "text analysis",
      "rstats"
    ],
    "contents": "\n\nContents\nBiden vs TrumpGet data\nTweet distribution\nWord frequency\nWordcloud\nOdds ratio\nRetweets and likes\nChanges in word use\n\n\n\n\n\nBiden vs Trump\nGet data\nThe datasource for Trump’s Twitter is from this amazing Twitter monitor website built by Brendan Brown, https://www.thetrumparchive.com/, and Biden’s Tiwtter is a Kaggle dataset build by Vopani, https://www.kaggle.com/rohanrao/joe-biden-tweets.\n\n\ntrump_tweets <- read_csv(\"tweets_11-06-2020.csv\")\nbiden_tweets <- read_csv(\"JoeBidenTweets.csv\")\n\n\n\nFirst, we will combine Trump’s and Biden’s Twitter dataset, and we will exclude retweets.\n\n\ntrump_tweets <- trump_tweets %>% \n  mutate(timestamp = date, tweet = text, likes = favorites) %>%\n  filter(!isRetweet) %>%\n  select(id, timestamp, tweet, retweets, likes)\n\nbiden_tweets <- biden_tweets %>% \n  select(-url, -replies, -quotes)\n\nbt_tweets <- bind_rows(trump_tweets %>%\n                         mutate(person = \"Trump\"),\n                       biden_tweets %>%\n                         mutate(person = \"Biden\")) %>%\n  mutate(timestamp = ymd_hms(timestamp))\n\nsummary(bt_tweets)\n\n\n       id              timestamp                      tweet          \n Min.   :3.614e+08   Min.   :2007-10-24 22:45:00   Length:51819      \n 1st Qu.:4.040e+17   1st Qu.:2013-11-22 20:27:13   Class :character  \n Median :6.646e+17   Median :2015-11-11 23:02:31   Mode  :character  \n Mean   :7.425e+17   Mean   :2016-06-13 01:01:13                     \n 3rd Qu.:1.144e+18   3rd Qu.:2019-06-26 18:41:20                     \n Max.   :1.325e+18   Max.   :2020-11-06 17:38:17                     \n    retweets          likes            person         \n Min.   :     0   Min.   :      0   Length:51819      \n 1st Qu.:    41   1st Qu.:     40   Class :character  \n Median :   827   Median :   1798   Mode  :character  \n Mean   :  7128   Mean   :  31276                     \n 3rd Qu.: 11586   3rd Qu.:  50812                     \n Max.   :408866   Max.   :1890946                     \n\nTweet distribution\nFrom the graph below you can see that Biden barely tweets and Trump is a lot more active on twitter. The two verticle line represent the announcement of entering presidential election, the first is Trump’s 2016 campaign, the second is Biden’s 2020 campaign.\n\n\ngroup.colors <- c(Trump = colors()[552], Biden = colors()[26])\n\nbt_tweets %>% ggplot(aes(timestamp, fill = person)) +\n  geom_histogram(position = \"identity\", bins = 30, show.legend = FALSE) +\n  geom_vline(xintercept = c(as.POSIXct(\"2015-06-16\", tz = \"UTC\"), as.POSIXct(\"2019-04-25\", tz = \"UTC\"))) +\n  scale_fill_manual(values = group.colors)+\n  facet_wrap(~person, ncol = 1)\n\n\n\n\nWord frequency\nIn order to make a tidy data frame ready for text anaylsis in the tweets, which we will remove retweets, the common English stop words, and any http links.\n\n\nremove_reg <- \"&amp;|&lt;|&gt;\" # to remove & < >\nbt_tweets_tidy<- bt_tweets %>%\n  filter(!str_detect(tweet, \"^RT\")) %>% # to remove retweet\n  mutate(text = str_remove_all(tweet, remove_reg),\n         text = str_remove_all(tweet, \"\\\\s?(f|ht)(tp)(s?)(://)([^\\\\.]*)[\\\\.|/](\\\\S*)\")) %>% # to remove http links\n  unnest_tokens(word, text, token = \"tweets\") %>%\n  filter(!word %in% stop_words$word, # to remove stop words \n         !word %in% str_remove_all(stop_words$word, \"'\"), # to remove ' in word\n         !word %in% c(\"amp\", \"lt\", \"gt\"), # remove amp/lt/gt in word\n         str_detect(word, \"[a-z]\")) \n\n\n\nNow we trying to create a data frame that count each words’ frequency for each person, first, we group by pearson, then count each words used by each person, then left join by a column with total words used by each person, then we can mutate a new column for frequency of each words.\n\n\nfrequ <- bt_tweets_tidy %>%\n  group_by(person) %>%\n  count(word, sort = TRUE) %>%\n  left_join(bt_tweets_tidy %>%\n              group_by(person) %>%\n              summarise(total = n())) %>%\n  mutate(freq = n/total)\nfrequ\n\n\n# A tibble: 53,915 x 5\n# Groups:   person [2]\n   person word                 n  total    freq\n   <chr>  <chr>            <int>  <int>   <dbl>\n 1 Trump  @realdonaldtrump  8121 384723 0.0211 \n 2 Trump  trump             5075 384723 0.0132 \n 3 Trump  president         3036 384723 0.00789\n 4 Trump  people            2941 384723 0.00764\n 5 Trump  country           2054 384723 0.00534\n 6 Trump  america           1850 384723 0.00481\n 7 Trump  donald            1778 384723 0.00462\n 8 Trump  time              1670 384723 0.00434\n 9 Trump  news              1511 384723 0.00393\n10 Trump  obama             1476 384723 0.00384\n# … with 53,905 more rows\n\nfrequ <- frequ %>% select(person, word, freq) %>%\n          spread(person, freq) %>%\n          arrange(Biden, Trump)\n\n\n\nFrom this graph below, the words show up near the read line indicate similar frequency in both Twitter account, the points toward the top mean the words show up more in Trump’s Twitter account, while the points toward the right mean the words show up more in Biden’s Twitter account. From this graph alone, we can already spot some vocabulary differances, such as “fake” & “equity”\n\n\nfrequ %>% ggplot(aes(Biden, Trump)) + \n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +\n  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  geom_abline(color = \"red\")\n\n\n\n\nHowever, Biden’s Twitter account is barely active prior to his announcement of 2020 election, so let’s see the frequency plot after 2019/04/25. Again, you see “fake” still falls above the redline and climate at the bottem right.\n\n\nbt_tweets_tidy_campagin <- bt_tweets_tidy %>% filter(timestamp >= as.Date(\"2019-04-25\"))\nfrequ_2019 <- bt_tweets_tidy_campagin %>%\n  group_by(person) %>%\n  count(word, sort = TRUE) %>%\n  left_join(bt_tweets_tidy %>%\n              group_by(person) %>%\n              summarise(total = n())) %>%\n  mutate(freq = n/total)\n\nfrequ_2019 <- frequ_2019 %>% select(person, word, freq) %>%\n  spread(person, freq) %>%\n  arrange(Biden, Trump)\n\n\n\n\n\nfrequ_2019 %>% ggplot(aes(Biden, Trump)) + \n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +\n  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  geom_abline(color = \"red\")\n\n\n\n\nWordcloud\n1. Total word counts\nThis graph represents the overall word counts in the dataset, closer to the center means that particular word appear the more than the other and the size of the word shows the magnitude of the word appearance, in this case “trump” and “president” are the most used word.\n\n\nbt_tweets_tidy %>%\n  group_by(word) %>%\n  filter(!str_detect(word, \"^@\")) %>%\n  count() %>%\nwith(wordcloud(word, n, min.freq = 200, max.word = 400, rot.per =0.35, random.order = FALSE,\n          colors = brewer.pal(12, \"Paired\"), scale = c(3,1)))\n\n\n\n\n2. Hashtags\nThese two graphs represent the hastag counts in Trump’s and Biden’s Twitter accounts, not surprisingly Trump’s most used hashtag is his 2016 presidential campaign “#trump2016”, whereas Biden is Democrat’s debate “#demdebate”, I would expect “teamjoe” be the most used hashtag.\n\n\nbt_tweets_tidy %>%\n  filter(person == \"Trump\") %>%\n  group_by(word) %>%\n  filter(str_detect(word, \"#\")) %>%\n  count() %>%\n  with(wordcloud(word,n, min.freq = 1, max.word = 300, rot.per =0.35, random.order = FALSE,\n                 colors = brewer.pal(12, \"Paired\"), scale = c(3,0.8)))\n\n\n\n\n\n\nbt_tweets_tidy %>%\n  filter(person == \"Biden\") %>%\n  group_by(word) %>%\n  filter(str_detect(word, \"#\")) %>%\n  count() %>%\n  with(wordcloud(word,n, min.freq = 1, max.word = 300, rot.per =0.35, random.order = FALSE,\n                 colors = brewer.pal(12, \"Paired\"), scale = c(3,0.8)))\n\n\n\n\n3. Separation in sentiments\nHere I tried a slightly fancier word cloud that separate the words base on the sentiments in NRC, https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html, graph 1 represents the sentiment word counts in Trump’s account, and graph 2 represents Biden’s. “vote”, “president” and “time” are all quite relevant in both account, but why “vote” is assigned to surprise is beyond my understanding.\n\n\n# trump words\nbt_tweets_tidy %>%\n  filter(person == \"Trump\" & !word == \"trump\") %>%\n  inner_join(get_sentiments(\"nrc\")) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  acast(word ~ sentiment, value.var = \"n\", fill =0) %>%\n  comparison.cloud(colors = brewer.pal(10, \"Paired\"), max.words = 400, \n                   match.colors = TRUE, title.size = 1.5, random.order = FALSE, scale = c(3,0.5))\n\n\n\n\n\n\n# biden words \nbt_tweets_tidy %>%\n  filter(person == \"Biden\" & !word == \"trump\") %>%\n  inner_join(get_sentiments(\"nrc\")) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  acast(word ~ sentiment, value.var = \"n\", fill =0) %>%\n  comparison.cloud(colors = brewer.pal(10, \"Paired\"), max.words = 400, \n                   match.colors = TRUE, title.size = 1.5, random.order = FALSE, scale = c(3,0.5))\n\n\n\n\nOdds ratio\nIn order to know which word is more likely to be used in either Twitter account, we will find the odds ratio for each words and here we use the timeframe after Biden announced his presidential campagin. After ungroup, we spread the data by person.\n\n\nbt_words_ratio <- bt_tweets_tidy_campagin %>%\n  filter(!str_detect(word, \"^@\")) %>% # remove tags on individual accounts \n  count(word, person) %>%\n  group_by(word) %>%\n  filter(sum(n) >= 10) %>% # only keep the words that are used more than 10 times \n  ungroup() %>%\n  spread(person, n, fill = 0) %>%\n  mutate_if(is.numeric, list(~(. + 1) / (sum(.) + 1))) %>%\n  mutate(logratio = log(Biden / Trump)) %>%\n  arrange(desc(logratio))\n\n\n\nHere it’s just a demonstration of how to calculate the odds ratio manunally.\n\n\nbt_tweets_tidy_campagin %>%\n  filter(!str_detect(word, \"^@\")) %>%\n  count(word, person) %>%\n  group_by(word) %>%\n  filter(sum(n) >= 10) %>%\n  ungroup() %>% # use ungroup here is because if use summarise the data would automatically reduce the dimension\n  spread(person, n, fill = 0) %>%\n  mutate(biden_t = sum(Biden),\n         trump_t = sum(Trump),\n         br = (Biden+1)/(biden_t+1), \n         tr = (Trump+1)/(trump_t+1), \n         logr = log(br/tr)) %>%\n  arrange(abs(logr))\n\n\n# A tibble: 2,488 x 8\n   word      Biden Trump biden_t trump_t       br       tr     logr\n   <chr>     <dbl> <dbl>   <dbl>   <dbl>    <dbl>    <dbl>    <dbl>\n 1 recovery     20    30   49467   72935 0.000425 0.000425 -0.00121\n 2 lot          43    64   49467   72935 0.000889 0.000891 -0.00194\n 3 words        44    66   49467   72935 0.000910 0.000919 -0.00977\n 4 happening    34    50   49467   72935 0.000708 0.000699  0.0118 \n 5 forgotten    10    15   49467   72935 0.000222 0.000219  0.0136 \n 6 july         10    15   49467   72935 0.000222 0.000219  0.0136 \n 7 pushing      10    15   49467   72935 0.000222 0.000219  0.0136 \n 8 refuse       10    15   49467   72935 0.000222 0.000219  0.0136 \n 9 task         10    15   49467   72935 0.000222 0.000219  0.0136 \n10 fighting     50    73   49467   72935 0.00103  0.00101   0.0160 \n# … with 2,478 more rows\n\nFrom the log odds ratio graph, I pick the top 15 words in both Twitter account, not surprisingly Biden’s account is more likely to use vocabulary such as “climate”, “gender”, “lgbtq”, “inclusive”, where as Trump’s account used a vocabulary like “wow”, “lamestream”, “sleepy”, “fake”, and most obviously “#maga”.\n\n\n## the words more likely from the otherside \ngroup_colors <- c(\"FALSE\" = colors()[26], \"TRUE\" = colors()[552]) \n\nbt_words_ratio %>%\n  group_by(logratio < 0) %>%\n  top_n(15, abs(logratio)) %>%\n  ungroup() %>%\n  mutate(word = reorder(word, logratio)) %>%\n  ggplot(aes(word, logratio, fill = `logratio < 0`)) +\n  geom_col(show.legend = FALSE) +\n  scale_fill_manual(values = group_colors) +\n  coord_flip() +\n  ylab(\"log odds ratio (Biden/Trump)\")\n\n\n\n\nRetweets and likes\n1. Retweets\nLet’s see what sort of words that give the highest retweets for Trump and Biden. First, check the total rtweets from both accounts. The first group_by() and summarise() count how many retweets of each tweets id, the second group_by() and sumarise() sum up the whole retweets for each account.\n\n\nbt_totals <- bt_tweets_tidy %>%\n  group_by(person, id) %>%\n  summarise(rts = first(retweets)) %>%\n  group_by(person) %>%\n  summarise(total_rts = sum(rts))\n\nbt_totals\n\n\n# A tibble: 2 x 2\n  person total_rts\n  <chr>      <dbl>\n1 Biden   37778753\n2 Trump  306628537\n\nNow we have the total retweets ready, we want to find the median of retweets of each word. The frist group_by() and summarise() show how many retweets does each word has for each tweet id and person. The second group_by() and summarise() show the median or retweets for each word and the total usage of each word. (so if the word is used only once, the median retweets is the same as the maximum and minum retweets.)\n\n\nbt_word_by_rts <- bt_tweets_tidy %>%\n  group_by(id, person, word) %>%\n  summarise(rts = first(retweets)) %>%\n  group_by(person, word) %>%\n  summarise(retweets = median(rts), uses =n()) %>%\n  left_join(bt_totals) %>%\n  filter(retweets != 0) %>%\n  ungroup()\n\nbt_word_by_rts\n\n\n# A tibble: 53,589 x 5\n   person word            retweets  uses total_rts\n   <chr>  <chr>              <dbl> <int>     <dbl>\n 1 Biden  @32bjseiu            263     1  37778753\n 2 Biden  @60minutes           652     1  37778753\n 3 Biden  @abby4iowa          1000     1  37778753\n 4 Biden  @abc                9296     1  37778753\n 5 Biden  @abcnetwork          751     1  37778753\n 6 Biden  @abeshinzo          7127     1  37778753\n 7 Biden  @adamsmithtimes       19     1  37778753\n 8 Biden  @adybarkan          6589     1  37778753\n 9 Biden  @aflcio              423     1  37778753\n10 Biden  @afscme               46     1  37778753\n# … with 53,579 more rows\n\n\n\ngroup.colors <- c(Trump = colors()[552], Biden = colors()[26])\n\nbt_word_by_rts %>%\n  filter(uses >= 5) %>%\n  group_by(person) %>%\n  top_n(10, retweets) %>%\n  arrange(retweets) %>%\n  ungroup() %>%\n  mutate(word = factor(word, unique(word))) %>%\n  ungroup() %>%\n  ggplot(aes(word, retweets, fill = person)) +\n  geom_col(show.legend = FALSE) +\n  scale_fill_manual(values = group.colors) +\n  facet_wrap(~person, scales = \"free\", ncol = 2) + \n  coord_flip() + \n  labs(x = NULL, y = \"Median # of retweets for tweets containing each word\")\n\n\n\n\n2. Likes\nThe strategy to get check the what word gives the highest median of retweets is the same as the one we used above.\n\n\nbt_totals_fav <- bt_tweets_tidy %>%\n  group_by(person, id) %>%\n  summarise(fav = first(likes)) %>%\n  group_by(person) %>%\n  summarise(total_favs = sum(fav))\n\nbt_totals_fav\n\n\n# A tibble: 2 x 2\n  person total_favs\n  <chr>       <dbl>\n1 Biden   206517140\n2 Trump  1320390366\n\n\n\nbt_word_by_fav <- bt_tweets_tidy %>%\n  group_by(id, person, word) %>%\n  summarise(fav = first(likes)) %>%\n  group_by(person, word) %>%\n  summarise(favs = median(fav), uses =n()) %>%\n  left_join(bt_totals_fav) %>%\n  filter(favs != 0) %>%\n  ungroup()\n\nbt_word_by_fav\n\n\n# A tibble: 53,149 x 5\n   person word             favs  uses total_favs\n   <chr>  <chr>           <dbl> <int>      <dbl>\n 1 Biden  @32bjseiu        1314     1  206517140\n 2 Biden  @60minutes       3705     1  206517140\n 3 Biden  @abby4iowa       6152     1  206517140\n 4 Biden  @abc            89310     1  206517140\n 5 Biden  @abcnetwork      7303     1  206517140\n 6 Biden  @abeshinzo      39912     1  206517140\n 7 Biden  @adamsmithtimes     7     1  206517140\n 8 Biden  @adybarkan      33607     1  206517140\n 9 Biden  @aflcio          1986     1  206517140\n10 Biden  @afscme             7     1  206517140\n# … with 53,139 more rows\n\n\n\nbt_word_by_fav %>%\n  filter(uses >= 5) %>%\n  group_by(person) %>%\n  top_n(10, favs) %>%\n  arrange(favs) %>%\n  ungroup() %>%\n  mutate(word = factor(word, unique(word))) %>%\n  ungroup() %>%\n  ggplot(aes(word, favs, fill = person)) +\n  geom_col(show.legend = FALSE) +\n  scale_fill_manual(values = group.colors) +\n  facet_wrap(~person, scales = \"free\", ncol = 2) + \n  coord_flip() + \n  labs(x = NULL, y = \"Median # of likes for tweets containing each word\")\n\n\n\n\nChanges in word use\nSo now we would look at how words’ frequencies change overtime. To do that, we first create a new time varible to get the unit of time for in order to calculate the word in that time span. Here, I choose 1 month as the base, in floor_date() you can set your unit of time in different measures. The count() counts how many times the word is use in 1 month then follow by a group_by() to account unit of time in order to use a mutate() to attach the total amount of word used in 1 month, the second group_by() accounts word so that mutate() can attach a total count of word use by that person.\n\n\nbt_words_by_time <- bt_tweets_tidy_campagin %>%\n  filter(!str_detect(word, \"^@\")) %>% # to filter out account taging in Twitter\n  mutate(time_floor = floor_date(timestamp, unit = \"1 month\")) %>%\n  count(time_floor, person, word) %>%\n  group_by(person, time_floor) %>%\n  mutate(time_total = sum(n)) %>%\n  group_by(person, word) %>%\n  mutate(word_total = sum(n)) %>%\n  ungroup() %>%\n  rename(count = n) %>%\n  filter(word_total > 50)\n\n\n\nNow the dataset is ready to be nested to run our model. Since we trying to find if the word useage change over time, it’s ideal to use a nested dataset, so in each word it nested a samller dataset that contains the “time_floor”, “count”, “time_total”, and “word_total”. Here we use glm and binomial for our model.\n\n\nbt_nested_data <- bt_words_by_time %>%\n  group_by(person, word) %>%\n  nest()\n\nbt_nested_data\n\n\n# A tibble: 507 x 3\n# Groups:   person, word [507]\n   person word       data             \n   <chr>  <chr>      <list>           \n 1 Biden  act        <tibble [19 × 4]>\n 2 Biden  address    <tibble [16 × 4]>\n 3 Biden  affordable <tibble [19 × 4]>\n 4 Biden  america    <tibble [20 × 4]>\n 5 Biden  american   <tibble [20 × 4]>\n 6 Biden  americans  <tibble [20 × 4]>\n 7 Biden  battle     <tibble [18 × 4]>\n 8 Biden  bring      <tibble [16 × 4]>\n 9 Biden  build      <tibble [20 × 4]>\n10 Biden  campaign   <tibble [19 × 4]>\n# … with 497 more rows\n\nmodel <- function(x){\n  glm(cbind(count, time_total) ~ time_floor, data = x, family = \"binomial\")\n}\n\n# map(bt_nested_data$data, model), I forgot how to use map() so put it here for reference \n\nbt_nested_models <- bt_nested_data %>%\n  mutate(models = map(data, model))\n\nbt_nested_models\n\n\n# A tibble: 507 x 4\n# Groups:   person, word [507]\n   person word       data              models\n   <chr>  <chr>      <list>            <list>\n 1 Biden  act        <tibble [19 × 4]> <glm> \n 2 Biden  address    <tibble [16 × 4]> <glm> \n 3 Biden  affordable <tibble [19 × 4]> <glm> \n 4 Biden  america    <tibble [20 × 4]> <glm> \n 5 Biden  american   <tibble [20 × 4]> <glm> \n 6 Biden  americans  <tibble [20 × 4]> <glm> \n 7 Biden  battle     <tibble [18 × 4]> <glm> \n 8 Biden  bring      <tibble [16 × 4]> <glm> \n 9 Biden  build      <tibble [20 × 4]> <glm> \n10 Biden  campaign   <tibble [19 × 4]> <glm> \n# … with 497 more rows\n\n### different way to run glm over nested data\n# bt_nested_models <- bt_nested_data %>%\n#  mutate(models = map(data, ~glm(cbind(count, time_total) ~ time_floor, ., family = \"binomial\")))\n###\n\n\n\nFrom above the bt_nested_models is has another list attach in the dataframe label as models, now we want to extract the componets in the glm model, here we can use a powerful function tidy() to summarize information in the model then unnest that models column. Since we are comparing multiple p values best to adjust them!\n\n\nslopes <- bt_nested_models %>%\n  mutate(models = map(models, tidy)) %>%\n  unnest(cols = c(models)) %>%\n  filter(term == \"time_floor\") %>%\n  mutate(adjusted.p.value = p.adjust(p.value))\n\ntop_slopes <- slopes %>% filter(adjusted.p.value <0.05)\n\n\n\nFrom the graphs below, both illustrate the trendy words that appeared more than 350 times in the period of 2019-04-25 to now. In the trump graph, it is obvious that closer to the election date the mentioning of biden and vote skyrocket compare to other words. In the other hand, biden graph shows that “trump” and “president” maintian rather stable high freqencies.\n\n\nbt_words_by_time %>%\n  inner_join(top_slopes, by = c(\"word\", \"person\")) %>%\n  filter(person == \"Trump\" &\n         word_total > 350) %>%\n  ggplot(aes(time_floor, count/time_total, color = word)) +\n  scale_color_brewer(palette = \"Set3\") + \n  geom_line(size = 1.3) +\n  labs(x = \"Trump\", y = \"Word frequency\", caption = \"Andrew Chen\")\n\n\n\nbt_words_by_time %>%\n  inner_join(top_slopes, by = c(\"word\", \"person\")) %>%\n  filter(person == \"Biden\" &\n           word_total > 350) %>%\n  ggplot(aes(time_floor, count/time_total, color = word)) +\n  scale_color_brewer(palette = \"Set3\") + \n  geom_line(size = 1.3) +\n  labs(x = \"Biden\", y = \"Word frequency\", caption = \"Andrew Cehn\")\n\n\n\n\n\n\n\n",
    "preview": "posts/welcome/welcome_files/figure-html5/tweet distribution-1.png",
    "last_modified": "2020-12-02T21:35:50+08:00",
    "input_file": "welcome.utf8.md",
    "preview_width": 1248,
    "preview_height": 768
  }
]
